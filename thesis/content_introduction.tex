% Introduction %
\chapter{Introduction}
\label{chapter1}
Performance analysis is one of the most important tools for developing high-quality software applications. For this purpose, the developed program code is either reviewed manually or the runtime of individual program sections is measured using performance counters within the code. This method is particularly essential in the field of high-performance computing and in the development of time-critical programs. However, performance and runtime can also be used to measure quality of all other applications. In \emph{OpenMP} and \emph{MPI} applications, the techniques profiling and tracing are used for performance analysis and consequently for performance improvement. As a result of the tracing method, all events that occur are visualized in a diagram, which enables a comprehensive analysis of the program code. However, the lightweight method of profiling also enables the user to find performance bottlenecks. This is usually accomplished by wrapping functions with performance counters and finally measuring the elapsed time within the functions at runtime. Thus now, the user can find out how often a function has been called and how much time each call takes. If more detailed information is needed, in most conventional profiling tools the user can determine code \ROI\emph{(ROI)} that are to be analyzed in more detail. For this, however, the user must have full knowledge of the underlying program code. Furthermore, the regions that are considered critical must be known before further analysis can be carried out. In order to address these problems, this thesis aims to develop an application that displays critical areas and allows immediate closer analysis on user-defined \roismall. 

\section{Motivation}
We have already explained that most conventional profiling tools work at the function level. This means that only larger sections are analyzed at a time, which results in information about the runtime of individual \STATS and \declssmall remaining hidden. Listing~\ref{lst:intro:example} shows a simple \CPP application in which a function \lstinline{loopFunc()} containing three \lstinline{for}-loops is called. 

\begin{lstlisting}[float, language=C++, caption=Example Code Showing a Simple \CPP Application., label=lst:intro:example]
void loopFunc() {
    for { /* code loop */ }
    for { /* code loop */ }
    for { /* code loop */ }
}

int main(void) {
    loopFunc();
    return 0;
}
\end{lstlisting} 

To analyze this program with the common profiler \emph{gprof}~\cite{gProfInformations}, we can compile the code with the help of~\emph{GCC}, a \emph{Compiler Collection} for various languages including \C and \CPP~\cite{GnuLanding}, as shown at the start of Listing~\ref{lst:intro:gprof statistic}. Afterwards, the application has to be executed so that analysis data can be collected. Finally, we can use \emph{gprof} to analyze the program~\cite{gprofworkflow}. It can be seen that the profiler returns runtime statistics for the \lstinline{loopFunc()} function, but misses detailed data about the separate \lstinline{for}-loops. Even if the user could find performance bottlenecks on the function level, there is no information about the individual loops or \DECL \STATS. However, the user could define specific code \roismall to particularize the output prior to the execution of the application~\cite{specifybeforrungprof}. To improve the profiling process for the user, we want to instrument the \roismall and implement them automatically. Therefore, the user could find performance bottlenecks not only on the function level, but also for individual \STATS or \declssmall. 

\begin{lstlisting}[float, language=C++, caption=The Compilation and Execution of the \emph{gprof} Profiler., label=lst:intro:gprof statistic]
gcc -pg -lstdc++ app.cpp -o app     // compilation of the program
./app                               // execution of the program
gprof app gmon.out                  // start runtime analysis

/* output generated by gprof */
Flat profile

Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ns/call  ns/call  name
100.00      0.01     0.01    50000    20.00    20.00  loopFunc()
  0.00      0.01     0.00        1     0.00     0.00  overhead
\end{lstlisting}

\section{Goal of the Thesis}
The aim of this thesis is to analyze bottlenecks in \CPP source codes in a programmatic way and therefore insert an individually adaptable performance counter automatically. For instance, we could assume that our application looks similar to the one in Listing~\ref{lst:intro:example}. An application based on the \CLANG~\cite{ClangLanding} \AST\emph{(AST)} transformation framework is to be developed that can analyze and transform the source code. The program should take a single-threaded \CPP application as input and finally return a program that contains performance counters wrapped around the \roismall. As shown in Listing~\ref{lst:intro:Inserted Performance Counter}, we can use our transformation tool not only to insert specific events at the function level, but also before and after specific instructions. From this point on, we will refer to the developed tool as \TOOL, which derives from \ROI\emph{Profiler}. The developed tool is freely available under the GitHub repository ``maxhagn/ROIProfilerCPP''~\cite{ROIPROFILER}.

\begin{lstlisting}[float, language=C++, caption={[Code Showing the Instructions Wrapped by Performance Counters.]Example Code Showing the Instructions Wrapped by Performance Counters.}, label=lst:intro:Inserted Performance Counter]
void loopFunc() {
    startEvent(1);              // inserted automatically
    for { /* code loop */ }
    endEvent(1);                // inserted automatically
    startEvent(2);              // inserted automatically
    for { /* code loop */ }
    endEvent(2);                // inserted automatically
    startEvent(3);              // inserted automatically
    for { /* code loop */ }
    endEvent(3);                // inserted automatically
}

int main(void) {
    startEvent(0);              // inserted automatically
    loopFunc();
    endEvent(0);                // inserted automatically
    printCollectedData();       // inserted automatically
    return 0;
}
\end{lstlisting}

Another specification of the \TOOL is that the transformation should be done hierarchically, which means that in the first transformation step only the outer scopes are wrapped, and in subsequent steps, also inner scopes are wrapped. Thus, the first time the \TOOL is executed, only the top level instructions should be measured in order to give the user first reference points for a performance analysis. The user receives a detailed list of the resources used within the program, with instructions returned as pairs with identification numbers. These statistics are shown in Table~\ref{tab:i:example_output_tool}, where it can be seen that in addition to the runtime of individual sections, information is also given about the proportion to the total resources used. Furthermore, the statistics also provide data about the resources used by the inserted performance counters. 

\begin{table}
  \centering
  \caption{The Desired Output of the Transformed Application.}
  \begin{tabular}{rrrrrr}
    \toprule
    Identifier & ClassType & Runtime                      & Scope \%              & Total \%              & Calls \\
    \midrule
    2192956    & ForStmt   & \SI{402.291}{\micro\second}  & \SI{4.62}{\percent}   & \SI{4.62}{\percent}   & 1     \\
    2193096    & ForStmt   & \SI{2.671}{\micro\second}    & \SI{0.03}{\percent}   & \SI{0.03}{\percent}   & 1     \\
    2225085    & ForStmt   & \SI{8303.364}{\micro\second} & \SI{95.34}{\percent}  & \SI{95.34}{\percent}  & 1     \\
    Overhead   &           & \SI{0.626}{\micro\second}    & < \SI{0.01}{\percent} & < \SI{0.01}{\percent} & 12    \\
    \midrule
    Runtime    &           & \SI{8709.478}{\micro\second} &                       &                       &       \\
    \bottomrule
  \end{tabular}
  \label{tab:i:example_output_tool}
\end{table}

If the user wants to obtain further statistics about a specific region, the \TOOL can be executed again, this time simply providing the identification numbers from the statistics. The method presented makes it possible to profile an application level by level. The generated statistics provide the user with an overview of particularly resource-intensive regions, which can be examined more closely through the incremental execution of the \TOOL. Thus, the \TOOL makes it possible to identify performance bottlenecks in software applications in a programmatic way. In order to prove that the developed \TOOL meets the specifications, a detailed functionality test and an overhead analysis are carried out. To check the desired functionality, applications of different complexity levels are profiled with the developed approach. In order to determine the overhead generated by the \TOOL, we run a set of programs before and after adding performance counters to check how the additional resource consumption influences the runtimes.

\section{Structure of the Work}
The thesis starts with a focus on the observation of \CLANG as the underlying framework in Chapter~\ref{chapter2}. Furthermore, the basic functionality of \LLVM as the back-end part of a compiler and \CLANG as the front-end framework is explained. To round off Chapter~\ref{chapter2}, we take a closer look at the representation used by \CLANG, as a fundamental understanding of it is essential for the development of our software. Chapter~\ref{chapter3} describes how the program code can be analyzed to find the desired locations in the application where the performance counters are to be positioned. The approach used to insert it dynamically and the possibilities that arise from this technique are described in Chapter~\ref{chapter4}. Furthermore, the runtime calculation and the output of the statistics are explained in this chapter. In Chapter~\ref{chapter5} we focus on the results of the research and further examine the performance of the developed tool more precisely. In order to enable an accurate evaluation, both a functionality test and an overhead analysis are carried out. In Chapter~\ref{chapter6} we address the limitations of the developed approach, especially by considering the possibilities for overhead reduction and the use for multi-threaded applications. Finally, we summarize all the findings of the research and clarify the need for further research and possible improvements.